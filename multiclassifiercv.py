# -*- coding: utf-8 -*-
"""MulticlassifierCV.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lYu99ae2D3wWFFVWIOIwmTlscr3D6yDy

Multiclassifier with cross validation
"""

import time
import numpy as np
import pandas as pd
from sklearn import model_selection
import csv
from datetime import datetime
from sklearn import metrics
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, cross_validate, cross_val_score
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier
from sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.dummy import DummyClassifier
from sklearn.feature_selection import SelectFromModel
from xgboost import plot_importance
from matplotlib import pyplot
from sklearn.metrics import  auc, confusion_matrix, mean_squared_error, plot_confusion_matrix
from sklearn.metrics import  roc_auc_score, roc_curve, classification_report, log_loss
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn import  neighbors
from sklearn.svm import SVC, NuSVC, LinearSVC
from sklearn.linear_model import SGDClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process import GaussianProcessClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.multiclass import OneVsRestClassifier, OutputCodeClassifier
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from warnings import simplefilter
from sklearn.model_selection import RepeatedKFold
# ignore all future warnings
simplefilter(action='ignore', category=FutureWarning)
simplefilter(action='ignore', category=Warning)

SEED = 123
NUM_FEATURES = 16
MAX_DEPTH = 4
MAX_ITER = 1000
N_NEIGHBORS = 5
accuracy_all = []
def calculateMetrics(results, model_name):

    print(">>> Metrics report")
    print(model_name)
    print("Cross validation score: {0:.2%} (+/- {1:.2%})".format(np.mean(results['test_accuracy']), np.std(results['test_accuracy']) * 2))
    print("precision score: {0:.2%} (+/- {1:.2%})".format(np.mean(results['test_precision']), np.std(results['test_precision']) * 2))
    print("recall score: {0:.2%} (+/- {1:.2%})".format(np.mean(results['test_recall']), np.std(results['test_recall']) * 2))
    print("f1_score: {0:.2%} (+/- {1:.2%})".format(np.mean(results['test_f1_score']), np.std(results['test_f1_score']) * 2))
   # print("auc_score: {0:.4} ".format(np.mean(results['test_roc_auc'])))
    #print("log loss:{0:.4}  ".format(np.mean(results['test_log_loss'])))
    return[]

def train_test_model(model_name, model, X, y):
    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=1)
    scoring = {'accuracy': make_scorer(accuracy_score), 'precision': make_scorer(precision_score, average='micro'),
               'recall': make_scorer(recall_score, average='micro'), 'f1_score': make_scorer(f1_score, average='micro')}
         #     'roc_auc': make_scorer(roc_auc_score, multi_class='ovr', needs_proba=True), 'log_loss': make_scorer(log_loss, needs_proba=True)}
    start = time.time()
    results = model_selection.cross_validate(model, X, y, cv=cv, scoring=scoring, n_jobs=-1)
    end = time.time()
    print("Training time: {0:.5} seconds \n".format(end - start))
    calculateMetrics(results, model_name)

def evaluateIndividualClassifiers(x, y):
    """
    evaluateIndividualClassifiers
        x : The features of the dataset to be used for predictions
        y : The target class for each row in "x"
        train_size_pct : {float in the range(0.0, 1.0)} the percentage of the dataset that should be used for training
    """
    max_depth_x2 = MAX_DEPTH * 2
    max_iter_x2 = MAX_ITER * 2
    n_neighbors_x2 = N_NEIGHBORS * 2
    n_neighbors_d2 = N_NEIGHBORS // 2

    rf = RandomForestClassifier(max_depth=MAX_DEPTH, criterion='entropy', random_state=SEED)
    rf_x2 = RandomForestClassifier(max_depth=max_depth_x2, criterion='entropy', random_state=SEED)
    et = ExtraTreesClassifier(max_depth=MAX_DEPTH, criterion='entropy', random_state=SEED)
    dectree = DecisionTreeClassifier(max_depth=MAX_DEPTH, random_state=SEED)
    knn = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)
    knn_x2 = KNeighborsClassifier(n_neighbors=n_neighbors_x2)
    knn_d2 = KNeighborsClassifier(n_neighbors=n_neighbors_d2)
    mlpnn = MLPClassifier(max_iter=MAX_ITER)
    mlpnnE = MLPClassifier(max_iter=MAX_ITER, early_stopping=True)
    mlpnn_x2 = MLPClassifier(max_iter=max_iter_x2)
    mlpnnE_x2 = MLPClassifier(max_iter=max_iter_x2, early_stopping=True)
    #XGB1=XGBClassifier()
    GNB1 = GaussianNB()
    dumm = DummyClassifier()
    knb = neighbors.KNeighborsClassifier()
    LR1 = LogisticRegression(max_iter=500)
    #SVC1=SVC(max_iter=500, kernel='rbf')
    #ovr1 = SGDClassifier(max_iter=max_iter_x2)
    ada1 = AdaBoostClassifier()
    gpc1 = GaussianProcessClassifier()
    GBclass1 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)
    histgclass = HistGradientBoostingClassifier(max_iter=100)
    bagclass = BaggingClassifier(MLPClassifier(), max_samples=0.5, max_features=0.5, n_jobs=-1)
    #ridge1 = RidgeClassifier(max_iter=100)
    Mnb = MultinomialNB()
    #SVC2 = NuSVC(max_iter=1000)
    #linear1=LinearSVC()

    classifier_mapping = {
        f'1-RandomForest case1-{MAX_DEPTH}' : rf,
        f'1-RandomForest case2-{max_depth_x2}' : rf_x2,
        f'2-ExtraTrees-{MAX_DEPTH}' : et,
        f'3-DecisionTree-{MAX_DEPTH}' : dectree,
        f'4-KNeighbors case1-{N_NEIGHBORS}' : knn,
        f'4-KNeighbors case2-{n_neighbors_x2}' : knn_x2,
        f'4-KNeighbors case3-{n_neighbors_d2}' : knn_d2,
        f'5-MLP case1-{MAX_ITER}' : mlpnn,
        f'5-MLP case2-{MAX_ITER}-early' : mlpnnE,
        f'5-MLP case3-{max_iter_x2}' : mlpnn_x2,
        f'5-MLP case4-{max_iter_x2}-early' : mlpnnE_x2,
        #f'6-XGB-' : XGB1,
        f'7-GNB-' : GNB1,
        f'8-dummy-' : dumm,
        f'9-knb-' : knb,
        f'10-LR1-' : LR1,
        #f'11-SVC1-' : SVC1,
        #f'12- ovr-'  : ovr1,
        f'13- ada-'  : ada1,
        f'14- gpc'  :  gpc1,
        f'15- GBclass': GBclass1,
        f'16- histgclas': histgclass,
        f'17- bagclas': bagclass,
        #f'18- ridge': ridge1,
        f'19- multinominal' : Mnb,
    }
    for model_name, model in classifier_mapping.items():

        train_test_model(model_name, model, x, y)

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/breast-cancer-wisconsin.data.txt')
df.replace('?',-99999, inplace=True)
df.drop(['id'], 1, inplace=True)

X = np.array(df.drop(['class'], 1))
y = np.array(df['class'])

# Look at the dataset again
print(f'Number of Rows: {df.shape[0]}')
print(f'Number of Columns: {df.shape[1]}')
print(df.head())

print(f'[*] Beginning evaluations: All Features')
evaluateIndividualClassifiers(X,y)